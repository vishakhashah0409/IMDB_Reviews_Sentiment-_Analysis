{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d47d8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re #used as a regular expression to find particular patterns and process it\n",
    "import sys\n",
    "#visualization library\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dc00ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Mountain Analytics IMDB Dataset 1.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5900ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ccc469",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe() # summary of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8706f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking number of positive and negative sentiment counts\n",
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f9d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum() #here we have total number of null values are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536b0565",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment'].unique() # find the unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0865786",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeling = {\n",
    "    'positive':1, \n",
    "    'negative':0\n",
    "}\n",
    "\n",
    "data['sentiment'] = data['sentiment'].apply(lambda x : labeling[x]) # it converts categarical sentiment into numerics\n",
    "# Output first ten rows\n",
    "data.head(10)\n",
    "#so,There are no missing values in any of the dataset's columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b94e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking how many duplicate valu there are?\n",
    "data.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1486942",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4566799d",
   "metadata": {},
   "source": [
    "Firstly we will use regular expressions to make the dataframe suitable for analysis.\n",
    "remove punctuation marks, remove HTML tags, remove URL's, remove characters which are not letters or digits, remove successive whitespaces, convert the text to lower case, strip whitespaces from the beginning and the end of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cae7eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping duplicate values from data\n",
    "data.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedce6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cheking duplicated values after droping\n",
    "data.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb89073",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328c70db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # importing libraries for cleanning text\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# function to clean whole text\n",
    "import re\n",
    "\n",
    "def remove_url(text):\n",
    "    url_tag = re.compile(r'https://\\S+|www\\.\\S+')\n",
    "    text = url_tag.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "def remove_html(text):\n",
    "    html_tag = re.compile(r'<.*?>')\n",
    "    text = html_tag.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "def remove_punctuation(text): \n",
    "    punct_tag = re.compile(r'[^\\w\\s]')\n",
    "    text = punct_tag.sub(r'', text) \n",
    "    return text\n",
    "\n",
    "def remove_special_character(text):\n",
    "    special_tag = re.compile(r'[^a-zA-Z0-9\\s]')\n",
    "    text = special_tag.sub(r'', text)\n",
    "    return text\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    return text\n",
    "    \n",
    "def clean_text(text):\n",
    "    text = remove_url(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_special_character(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a745e6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['processed'] = data['review'].apply(lambda x: clean_text(x)) # how the data looks like now\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2325d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove any sequence of digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Other cleaning operations\n",
    "    # ...\n",
    "    return text\n",
    "#Define the regular expression for numbers\n",
    "num_regex = re.compile(r'\\d+')\n",
    "\n",
    "# Clean the review text by removing numbers\n",
    "data['processed'] = data['review'].apply(lambda x: num_regex.sub('', x))\n",
    "data['processed'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7abafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    # Remove any sequence of digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Other cleaning operations\n",
    "    # ...\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6877b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization of text\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer=ToktokTokenizer() #for every function\n",
    "\n",
    "#Setting English stopwords\n",
    "stopword_list=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366aa321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n",
    "#we can either remove stopwords before or after stemming. But since this is a review context, we expect users to have used many different words and we did\n",
    "# stemming before filtering for stopwords.\n",
    "\n",
    "stop=set(stopwords.words('english'))\n",
    "print(stop)\n",
    "\n",
    "#Removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "#Apply function on review column\n",
    "data['review']=data['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347312b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming and Lemmatization\n",
    "#Stemming is rule-based, it omits the last few letters like 'ing', 'ed', 'es' and more. It is fast but may create strange words. Lemmatizing is dictionary-based, where it translates all words to the root form, like 'went' to 'go', 'going' to 'go' and more. Generally we prefer lemmatizing, but it might take some time in large datasets.\n",
    "from nltk.stem import WordNetLemmatizer,SnowballStemmer\n",
    "def simple_stemmer(text):\n",
    "    ps = SnowballStemmer(language='english')\n",
    "    return ' '.join([ps.stem(word) for word in tokenizer.tokenize(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ed2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_stemmer(text):\n",
    "    ps = SnowballStemmer(language='english')\n",
    "    return ' '.join([ps.stem(word) for word in tokenizer.tokenize(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee98b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['processed'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723b6a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time simple_stemmer(data['processed'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8611a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "def lemmatize_all(sentence):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(word_tokenize(sentence)):\n",
    "        if tag.startswith(\"NN\"):\n",
    "            yield wnl.lemmatize(word, pos='n')\n",
    "        elif tag.startswith('VB'):\n",
    "            yield wnl.lemmatize(word, pos='v')\n",
    "        elif tag.startswith('JJ'):\n",
    "            yield wnl.lemmatize(word, pos='a')\n",
    "        else:\n",
    "            yield word\n",
    "            \n",
    "def lemmatize_text(text):\n",
    "    return ' '.join(lemmatize_all(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b20bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['processed'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf41b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time lemmatize_text(data['processed'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9ad618",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27169fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it shows the distribution of sentiments\n",
    "data.groupby('sentiment').count().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321b349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from matplotlib import style\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c97b48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def no_of_words(text):\n",
    "    #words= text.split()\n",
    "   # word_count = len(words)\n",
    "   # return word_count\n",
    "\n",
    "#data['word count'] = data['review'].apply(no_of_words)\n",
    "#data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35669c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c54373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate review lengths\n",
    "review_len = pd.Series([len(review.split()) for review in data['review']])\n",
    "\n",
    "# The distribution of review text lengths\n",
    "review_len.plot(kind='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8420ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now,visualize how long our sentences are in the training data.\n",
    "sns.set_theme(\n",
    "    context='notebook',\n",
    "    style='darkgrid',\n",
    "    palette='deep',\n",
    "    font='sans-serif',\n",
    "    font_scale=1,\n",
    "    color_codes=True,\n",
    "    rc=None,\n",
    ")\n",
    "\n",
    "plt.figure(figsize = (10,12))\n",
    "sns.histplot(review_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b2e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,7))\n",
    "data['length'] = data.review.str.split().apply(len)\n",
    "ax1 = fig.add_subplot(122)\n",
    "sns.histplot(data[data['sentiment']==1]['length'], ax=ax1,color='green')\n",
    "describe = data.length[data.sentiment==1].describe().to_frame().round(2)\n",
    "\n",
    "ax2 = fig.add_subplot(121)\n",
    "ax2.axis('off')\n",
    "font_size = 14\n",
    "bbox = [0, 0, 1, 1]\n",
    "table = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\n",
    "table.set_fontsize(font_size)\n",
    "fig.suptitle('Distribution of text length for positive sentiment reviews.', fontsize=16)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03ccc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,7))\n",
    "ax1 = fig.add_subplot(122)\n",
    "sns.histplot(data[data['sentiment']==0]['length'], ax=ax1,color='red')\n",
    "describe = data.length[data.sentiment==0].describe().to_frame().round(2)\n",
    "\n",
    "ax2 = fig.add_subplot(121)\n",
    "ax2.axis('off')\n",
    "font_size = 14\n",
    "bbox = [0, 0, 1, 1]\n",
    "table = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\n",
    "table.set_fontsize(font_size)\n",
    "fig.suptitle('Distribution of text length for Negative sentiment reviews.', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37cc990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the frequent words\n",
    "\n",
    "all_words = \" \".join([sentence for sentence in data['processed']])\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words)\n",
    "\n",
    "# plot the graph\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26db7dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequent words visualization for -ve\n",
    "\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "plt.figure(figsize = (15,8)) # Negative Review Text\n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800, random_state=42).generate(\" \".join(data[data.sentiment == 0].processed))\n",
    "plt.imshow(wc , interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67be8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequent words visualization for +ve\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "plt.figure(figsize = (15,8)) # Positive Review Text\n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(data[data.sentiment == 1].processed))\n",
    "plt.imshow(wc , interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efaa95e",
   "metadata": {},
   "source": [
    "# prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e369c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment'] = data['sentiment'].map({'positive':1,'negative':0})\n",
    "\n",
    "train_data = data.sample(frac=0.8,random_state=100)\n",
    "test_data = data.drop(train_data.index)\n",
    "\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Test  data shape: {test_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6448729",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a9ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "tokenizer  = tf.keras.preprocessing.text.Tokenizer(num_words=8000)\n",
    "tokenizer.fit_on_texts(np.append(train_data['review'].values,test_data['review'].values))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = len(word_index) + 1\n",
    "\n",
    "train_seq = tokenizer.texts_to_sequences(train_data[\"review\"])\n",
    "test_seq = tokenizer.texts_to_sequences(test_data[\"review\"])\n",
    "\n",
    "train_data = tf.keras.preprocessing.sequence.pad_sequences(train_seq, maxlen=100)\n",
    "test_data = tf.keras.preprocessing.sequence.pad_sequences(test_seq, maxlen=100)\n",
    "\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Test  data shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d1662a",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e10e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03015418",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058abf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset  \n",
    "#train dataset\n",
    "train_reviews=data.review[:40000]\n",
    "train_sentiments=data.sentiment[:40000]\n",
    "#test dataset\n",
    "test_reviews=data.review[40000:]\n",
    "test_sentiments=data.sentiment[40000:]\n",
    "print(train_reviews.shape,train_sentiments.shape)\n",
    "print(test_reviews.shape,test_sentiments.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bad7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['review']\n",
    "Y = data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbd319d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
