{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "917a82a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\visha\\anaconda3\\lib\\site-packages (4.3.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\visha\\anaconda3\\lib\\site-packages (from gensim) (1.24.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\visha\\anaconda3\\lib\\site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\visha\\anaconda3\\lib\\site-packages (from gensim) (1.7.1)\n",
      "Collecting numpy>=1.18.5\n",
      "  Downloading numpy-1.22.4-cp39-cp39-win_amd64.whl (14.7 MB)\n",
      "     --------------------------------------- 14.7/14.7 MB 36.3 MB/s eta 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.3.0 requires daal==2021.2.3, which is not installed.\n",
      "numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.22.4 which is incompatible.\n",
      "en-core-web-lg 3.3.0 requires spacy<3.4.0,>=3.3.0.dev0, but you have spacy 3.5.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.2\n",
      "    Uninstalling numpy-1.24.2:\n",
      "      Successfully uninstalled numpy-1.24.2\n",
      "Successfully installed numpy-1.22.4\n",
      "Requirement already satisfied: numpy in c:\\users\\visha\\anaconda3\\lib\\site-packages (1.22.4)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.24.2-cp39-cp39-win_amd64.whl (14.9 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.4\n",
      "    Uninstalling numpy-1.22.4:\n",
      "      Successfully uninstalled numpy-1.22.4\n",
      "Successfully installed numpy-1.24.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.3.0 requires daal==2021.2.3, which is not installed.\n",
      "scipy 1.7.1 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.24.2 which is incompatible.\n",
      "numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.24.2 which is incompatible.\n",
      "en-core-web-lg 3.3.0 requires spacy<3.4.0,>=3.3.0.dev0, but you have spacy 3.5.0 which is incompatible.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20d5778c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** --> Modules are imported: \n",
      "Python version: 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]\n",
      "numpy version: 1.24.2\n",
      "pandas version: 1.5.3\n",
      "sklearn version: 0.24.2\n",
      "nltk version: 3.8.1\n",
      "gensim version: 4.3.1\n"
     ]
    }
   ],
   "source": [
    "# data processing and Data manipulation\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Libraries and packages for text (pre-)processing \n",
    "import string\n",
    "import re\n",
    "import nltk \n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# For type hinting\n",
    "from typing import List    \n",
    "# Libraries and packages for NLP\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Visualization \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "matplotlib.rcParams['axes.labelsize'] = 14\n",
    "matplotlib.rcParams['xtick.labelsize'] = 12\n",
    "matplotlib.rcParams['figure.figsize'] = (12, 10)\n",
    "matplotlib.rcParams['ytick.labelsize'] = 12\n",
    "matplotlib.rcParams['text.color'] = 'k'\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "print('*** --> Modules are imported: ')    \n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"numpy version:\", np.__version__)\n",
    "print(\"pandas version:\", pd.__version__)\n",
    "\n",
    "print(\"sklearn version:\", sklearn.__version__)\n",
    "print(\"nltk version:\", nltk.__version__)\n",
    "print(\"gensim version:\", gensim.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2735d78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Importing IMDB Data from data directory which is two directory uper than the current directory\n",
    "data_path = os.path.abspath(os.path.join(os.pardir, \n",
    "                                         os.pardir, \n",
    "                                         'Mountain Analytics IMDB Dataset 1.csv'))\n",
    "data = pd.read_csv(data_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cdf99a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Tokenizer: \n",
    "    \"\"\" After cleaning and denoising steps, in this class the text is broken up into tokens.\n",
    "    if clean: clean the text from all non-alphanumeric characters,\n",
    "    if lower: convert the text into lowercase,\n",
    "    If de_noise: remove HTML and URL components,\n",
    "    if remove_stop_words: and remove stop-words,\n",
    "    If keep_neagation: attach the negation tokens to the next token \n",
    "     and treat them as a single word before removing the stopwords\n",
    "     \n",
    "    Returns:\n",
    "    List of tokens\n",
    "    \"\"\"\n",
    "    # initialization method to create the default instance constructor for the class\n",
    "    def __init__(self,\n",
    "                 clean: bool = True,\n",
    "                 lower: bool = True, \n",
    "                 de_noise: bool = True, \n",
    "                 remove_stop_words: bool = True,\n",
    "                keep_negation: bool = True) -> List[str]:\n",
    "      \n",
    "        self.de_noise = de_noise\n",
    "        self.remove_stop_words = remove_stop_words\n",
    "        self.clean = clean\n",
    "        self.lower = lower\n",
    "        self.stopwords = nltk.corpus.stopwords.words('english')\n",
    "        self.keep_negation = keep_negation\n",
    "\n",
    "    # other methods  \n",
    "    def denoise(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Removing html and URL components\n",
    "        \"\"\"\n",
    "        html_pattern = r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\"\n",
    "        url_pattern = r\"((http://)[^ ]*|(https://)[^ ]*|(www\\.)[^ ]*)\"\n",
    "\n",
    "        text = re.sub(html_pattern, \" \", text)\n",
    "        text = re.sub(url_pattern,\" \",text).strip()\n",
    "        return text\n",
    "       \n",
    "    \n",
    "    def remove_stopwords(self, tokenized_text: List[str]) -> List[str]:\n",
    "        text = [word for word in tokenized_text if word not in self.stopwords]\n",
    "        return text\n",
    "\n",
    "    \n",
    "    def keep_negation_sw(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        A function to save negation words (n't, not, no) from removing as stopwords\n",
    "        \"\"\"\n",
    "        # to replace \"n't\" with \"not\"\n",
    "        text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "        text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "        text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "        # to join not/no into the next word\n",
    "        text = re.sub(\"not \", \" NOT\", text)\n",
    "        text = re.sub(\"no \", \" NO\", text)\n",
    "        return text\n",
    "    \n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        A function to tokenize words of the text\n",
    "        \"\"\"\n",
    "        non_alphanumeric_pattern =r\"[^a-zA-Z0-9]\"\n",
    "        \n",
    "        # to substitute multiple whitespace with single whitespace\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        if self.de_noise:\n",
    "            text = self.denoise(text)\n",
    "        if self.lower:\n",
    "            text = text.lower()\n",
    "        if self.keep_negation:\n",
    "            text = self.keep_negation_sw(text)\n",
    "            \n",
    "        if self.clean:\n",
    "            # to remove non-alphanumeric characters\n",
    "            text = re.sub(non_alphanumeric_pattern,\" \", text).strip()\n",
    "\n",
    "        tokenized_text = text.split()\n",
    "\n",
    "        if self.remove_stop_words:\n",
    "            tokenized_text = self.remove_stopwords(tokenized_text)\n",
    "\n",
    "        return tokenized_text\n",
    "\n",
    "    \n",
    "    \n",
    "def evaluate_model (y_true: pd.Series, \n",
    "                              y_pred: pd.Series, \n",
    "                              report:bool = False,\n",
    "                              plot: bool = False)-> float:\n",
    "    \"\"\"\n",
    "    A function to calculate F1, Accuracy, Recall, and Precision Score\n",
    "    If report: it prints classification_report \n",
    "    If plot: it prints Confusion Matrix Heatmap\n",
    "    \"\"\"\n",
    "    if report:\n",
    "        print(classification_report(y_true, \n",
    "                            y_pred,\n",
    "                            digits=4))\n",
    "    if plot:\n",
    "        # figure\n",
    "        fig, ax = plt.subplots(figsize=(4, 4))\n",
    "        conf_matrix = pd.crosstab(y_true, \n",
    "                           y_pred, \n",
    "                           rownames=['Actual'], \n",
    "                           colnames=['Predicted'])\n",
    "        sns.heatmap(conf_matrix, \n",
    "                    annot=True, fmt=\".0f\",\n",
    "                    cmap='RdYlGn', # use orange/red colour map\n",
    "                    cbar_kws={'fraction' : 0.04}, # shrink colour bar\n",
    "                    linewidth=0.3, # space between cells\n",
    "                   ) \n",
    "        plt.title('Confusion Matrix', fontsize=14)\n",
    "        plt.show()\n",
    "        \n",
    "    if not report and not plot:\n",
    "        print('* Accuracy Score: ', \"{:.4%}\".format(accuracy_score(y_true, y_pred)))\n",
    "        print('* F1 Score: ', \"{:.4%}\".format(f1_score(y_true, y_pred )))\n",
    "        print('* Recall Score: ', \"{:.4%}\".format(recall_score(y_true , y_pred )))\n",
    "        print('* Precision Score: ', \"{:.4%}\".format(precision_score(y_true , y_pred)))\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def bow_vectorizer(doc_tokens: List[str]):\n",
    "    \"\"\"\n",
    "    Using CountVectorizer, this function converts a list of tokenized text documents to a matrix of token counts (a Bog-of-Words sparse matrix).\n",
    "    \n",
    "    Parameters:\n",
    "    doc_tokens               : A tokenized document \n",
    "    Returns:\n",
    "    fit_bow_count_vect       : A fit Bog-of-Words model\n",
    "    \"\"\"\n",
    "    # Defining CountVectorizer\n",
    "    count_vect = CountVectorizer(\n",
    "        analyzer='word',\n",
    "        tokenizer=lambda doc:doc,\n",
    "        preprocessor=lambda doc:doc,\n",
    "        min_df=5,\n",
    "        token_pattern=None)\n",
    "    \n",
    "    # Create a sparse matrix out of the frequency of vocabulary words in Train Dataset\n",
    "    fit_bow_count_vect = count_vect.fit(doc_tokens)\n",
    "    \n",
    "    return fit_bow_count_vect\n",
    "    \n",
    "    \n",
    "\n",
    "def train_logistic_regressor(train_data_bow_matrix, \n",
    "                             train_data_label: pd.Series):\n",
    "    \"\"\"\n",
    "    This function builds a LogisticRegressionCV Classifier Model with Bag-of-Words matrix of the Train dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    train_data_bow_matrix    : A Train dataset as Bog-of-Words sparse matrix \n",
    "    train_data_label         : Target values of the Train dataset\n",
    "    Returns:\n",
    "    bow_logistreg_model      : A fit LogisticRegression model on Bag-of_words vectors\n",
    "    \"\"\"\n",
    "    bow_logistreg_model=LogisticRegressionCV(cv=5,\n",
    "                                             random_state=42,\n",
    "                                             n_jobs=-1,\n",
    "                                             verbose=3,\n",
    "                                             max_iter=300).fit(train_data_bow_matrix, train_data_label)\n",
    "    \n",
    "    train_data_predict_label = bow_logistreg_model.predict(train_data_bow_matrix)\n",
    "    \n",
    "    print(\"==> Evaluation metrics on training data: \")\n",
    "    evaluate_model(y_true = train_data_label, \n",
    "                   y_pred = train_data_predict_label)\n",
    "    return bow_logistreg_model\n",
    "\n",
    "\n",
    "def w2v_trainer(doc_tokens: List[str],\n",
    "                epochs: int = 10,\n",
    "                workers: int = 3,\n",
    "                vector_size: int = 300,\n",
    "                window: int = 5,\n",
    "                min_count: int = 2):\n",
    "    \"\"\" \n",
    "    Going through a list of lists, where each list within the main list contains a set of tokens from a doc, this function trains a Word2Vec model,\n",
    "    then creates two objects to store keyed vectors and keyed vocabs   \n",
    "    Parameters:\n",
    "    doc_tokens   : A tokenized document \n",
    "    epochs       : Number of epochs training over the corpus\n",
    "    workers      : Number of processors (parallelization)\n",
    "    vector_size  : Dimensionality of word embeddings\n",
    "    window       : Context window for words during training\n",
    "    min_count    : Ignore words that appear less than this\n",
    "    Returns:\n",
    "    keyed_vectors       : A word2vec vocabulary model\n",
    "    keyed_vocab \n",
    "    \n",
    "    \"\"\"\n",
    "    w2v_model = Word2Vec(doc_tokens,\n",
    "                         epochs=10,\n",
    "                         workers=3,\n",
    "                         vector_size=300,\n",
    "                         window=5,\n",
    "                         min_count=2)\n",
    "    \n",
    "    # create objects to store keyed vectors and keyed vocabs\n",
    "    keyed_vectors = w2v_model.wv\n",
    "    keyed_vocab = keyed_vectors.key_to_index\n",
    "    \n",
    "    return keyed_vectors, keyed_vocab\n",
    "    \n",
    "    \n",
    "    \n",
    "def calculate_overall_similarity_score(keyed_vectors,\n",
    "                             target_tokens: List[str], \n",
    "                             doc_tokens: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Going through a tokenized doc, this function computes vector similarity between \n",
    "    doc_tokens and target_tokens as 2 lists by n_similarity(list1, list2) method based on \n",
    "    Word2Vec vocabulary (keyed_vectors), \n",
    "    then returns the similarity scores.  \n",
    "    \n",
    "    Parameters:\n",
    "    target_tokens  : A set of semantically co-related words  \n",
    "    doc_tokens     : A tokenized document \n",
    "    keyed_vectors  : A word2vec vocabulary model\n",
    "    \n",
    "    Returns:\n",
    "    vector similarity scores between 2 tokenized list doc_tokens and target_tokens  \n",
    "    \"\"\"\n",
    "    \n",
    "    target_tokens = [token for token in target_tokens if token in keyed_vectors]\n",
    "\n",
    "    doc_tokens = [token for token in doc_tokens if token in keyed_vectors]\n",
    "    \n",
    "    similarity_score = keyed_vectors.n_similarity(target_tokens, doc_tokens)\n",
    "    \n",
    "    return similarity_score\n",
    "\n",
    "\n",
    "def overall_semantic_sentiment_analysis (keyed_vectors, \n",
    "                                         positive_target_tokens: List[str],\n",
    "                                         negative_target_tokens: List[str],\n",
    "                                         doc_tokens: List[str], \n",
    "                                         doc_is_series: bool = True) -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the semantic sentiment of the text. \n",
    "    It first computes a  vector for the text (average of the  wordvectors building the text document vector)\n",
    "    and two vectors representing our given positive and negative lists of words \n",
    "    and then calculates Positive and Negative Sentiment Scores as cosine similarity \n",
    "    between the text vector and the positive and negative vectors respectively.\n",
    "    \n",
    "    Parameters:\n",
    "    keyed_vectors           : A word2vec vocabulary model\n",
    "    positive_target_tokens  : A list of sentiment or opinion words that indicate positive opinions \n",
    "    negative_target_tokens  : A list of sentiment or opinion words that indicate negative opinions  \n",
    "    doc_tokens              : A tokenized document \n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    positive_score : vector similarity scores between doc_tokens and positive_target_tokens\n",
    "    negative_score : vector similarity scores between doc_tokens and negative_target_tokens\n",
    "    \n",
    "    semantic_sentiment_score  : positive_score - negative_score\n",
    "    semantic_sentiment_polarity : Overall score: 0 for more negative or 1 for more positive doc\n",
    "    \"\"\"\n",
    "  \n",
    "    positive_score = doc_tokens.apply(lambda x: calculate_overall_similarity_score(keyed_vectors=keyed_vectors, \n",
    "                                                                 target_tokens=positive_target_tokens, \n",
    "                                                                 doc_tokens=x))\n",
    "\n",
    "    negative_score = doc_tokens.apply(lambda x: calculate_overall_similarity_score(keyed_vectors=keyed_vectors, \n",
    "                                                                 target_tokens=negative_target_tokens, \n",
    "                                                                 doc_tokens=x))\n",
    "\n",
    "    semantic_sentiment_score = positive_score - negative_score\n",
    "    \n",
    "    semantic_sentiment_polarity = semantic_sentiment_score.apply(lambda x: 1 if (x > 0) else 0)\n",
    "                                          \n",
    "    return positive_score, negative_score, semantic_sentiment_score, semantic_sentiment_polarity\n",
    "\n",
    "\n",
    "def list_similarity(keyed_vectors, \n",
    "                    wordlist1: List[str], \n",
    "                    wordlist2: List[str]) -> pd.Series:\n",
    "    \"\"\" A function to calculate vector similarity between 2 lists of tokens\"\"\"\n",
    "    wv1= np.array([keyed_vectors[wd] for wd in wordlist1 if wd in keyed_vectors])\n",
    "    wv2= np.array([keyed_vectors[wd] for wd in wordlist2 if wd in keyed_vectors])\n",
    "    wv1 /= np.linalg.norm(wv1, axis=1)[:, np.newaxis]\n",
    "    wv2 /= np.linalg.norm(wv2, axis=1)[:, np.newaxis]\n",
    "\n",
    "    return np.dot(wv1, np.transpose(wv2))\n",
    "    \n",
    "\n",
    "def calculate_topn_similarity_score(keyed_vectors, \n",
    "                          target_tokens: List[str], \n",
    "                          doc_tokens: List[str],\n",
    "                          topn: int = 10) -> float:\n",
    "    \"\"\" The function defines the similarity of a single word to a document, \n",
    "    as the average of its similarity with the top_n most similar words in that document. \n",
    "    To calculate the similarity score it calculates the similarity of every word in the target_tokens set with all the words in the doc_tokens, \n",
    "    and keeps the top_n highest scores for each word and then averages over all the kept scores.\n",
    "    -----\n",
    "    Parameters:\n",
    "    target_tokens List[str] : A list of sentiment or opinion words that indicate negative or positive opinions  \n",
    "    \n",
    "    doc_tokens List[str]    : A tokenized document \n",
    "    \n",
    "    keyed_vectors           : A word2vec vocabulary model\n",
    "    \n",
    "    topn (int)              : An int that indicates the number of\n",
    "    most similar vectors used to calculate the similarity score.\n",
    "    \n",
    "    Returns:\n",
    "    vector similarity scores between 2 tokenized list doc_tokens and target_tokens  \n",
    "    \"\"\"\n",
    "    topn = min(topn, round(len(doc_tokens)))\n",
    "    \n",
    "    target_tokens = [token for token in target_tokens if token in keyed_vectors]\n",
    "\n",
    "    doc_tokens = [token for token in doc_tokens if token in keyed_vectors]\n",
    "    \n",
    "    sim_matrix = list_similarity(keyed_vectors=keyed_vectors, \n",
    "                                 wordlist1=target_tokens,\n",
    "                                 wordlist2=doc_tokens)\n",
    "    sim_matrix = np.sort(sim_matrix, axis=1)[:, -topn:]\n",
    "     \n",
    "    similarity_score = np.mean(sim_matrix)\n",
    "    \n",
    "    return similarity_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def topn_semantic_sentiment_analysis(keyed_vectors, \n",
    "                                      positive_target_tokens: List[str],\n",
    "                                      negative_target_tokens: List[str],\n",
    "                                      doc_tokens: List[str],\n",
    "                                      topn: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    A function to calculate the semantic sentiment of the text by measuring vector similarity between \n",
    "    doc_tokens and a positive_target_tokens (as positive_score) then measuring vector similarity between \n",
    "    doc_tokens and a negative_target_tokens (as negative_score), and finally comparing these two scores. \n",
    "    \n",
    "    Parameters:\n",
    "    keyed_vectors           : A word2vec vocabulary model\n",
    "    positive_target_tokens  : A list of sentiment or opinion words that indicate positive opinions \n",
    "    negative_target_tokens  : A list of sentiment or opinion words that indicate negative opinions  \n",
    "    doc_tokens              : A tokenized document \n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    positive_score            : vector similarity scores between doc_tokens and positive_target_tokens\n",
    "    negative_score            : vector similarity scores between doc_tokens and negative_target_tokens\n",
    "    \n",
    "    semantic_sentiment_score  : positive_score - negative_score\n",
    "    semantic_sentiment_polarity       : Overall score: 0 for more negative or 1 for more positive doc\n",
    "    \"\"\"\n",
    "  \n",
    "    positive_score = doc_tokens.apply(lambda x: calculate_topn_similarity_score(keyed_vectors=keyed_vectors, \n",
    "                                                                 target_tokens=positive_target_tokens, \n",
    "                                                                 doc_tokens=x,\n",
    "                                                                     topn=topn))\n",
    "                                      \n",
    "    negative_score = doc_tokens.apply(lambda x: calculate_topn_similarity_score(keyed_vectors=keyed_vectors, \n",
    "                                                                 target_tokens=negative_target_tokens, \n",
    "                                                                 doc_tokens=x,\n",
    "                                                                     topn=topn))\n",
    "                                           \n",
    "    semantic_sentiment_score = positive_score - negative_score\n",
    "    \n",
    "    semantic_sentiment_polarity = semantic_sentiment_score.apply(lambda x: 1 if (x > 0) else 0)\n",
    "                                          \n",
    "    return positive_score, negative_score, semantic_sentiment_score, semantic_sentiment_polarity\n",
    "\n",
    "\n",
    "def text_SSA(keyed_vectors,\n",
    "              tokenizer,\n",
    "              positive_target_tokens: List[str],\n",
    "              negative_target_tokens: List[str],\n",
    "              topn: int = 30) -> float:\n",
    "    \"\"\"\n",
    "    A function to analyze text semantic sentiment.\n",
    "    \"\"\"\n",
    "    repeat = True\n",
    "    while repeat:\n",
    "        txt = input(\"Please insert text here: \\n\")\n",
    "        tokenized_txt = tokenizer.tokenize(txt)\n",
    "        txt_PSS = calculate_topn_similarity_score(keyed_vectors=keyed_vectors, \n",
    "                                    target_tokens=positive_target_tokens, \n",
    "                                    doc_tokens=tokenized_txt,\n",
    "                                    topn=topn)\n",
    "\n",
    "        txt_NSS = calculate_topn_similarity_score(keyed_vectors=keyed_vectors, \n",
    "                                    target_tokens=negative_target_tokens, \n",
    "                                    doc_tokens=tokenized_txt,\n",
    "                                    topn=topn)\n",
    "\n",
    "        txt_S3 = txt_PSS - txt_NSS\n",
    "        semantic_sentiment_polarity = \"Positive\" if (txt_S3 >= 0) else \"Negative\"\n",
    "        green = \"\\033[1;32m\"\n",
    "        red = \"\\033[1;31m\"\n",
    "        color = green if (txt_S3 >= 0) else red # to print colored text\n",
    "\n",
    "        print(\"Tokenized text:\\n \", tokenizer.tokenize(txt))\n",
    "        print(\"PSS =\", txt_PSS)\n",
    "        print(\"NSS =\", txt_NSS)\n",
    "        print(\"S3 =\", txt_S3)\n",
    "        print(color + \"Semantic Sentiment =\", semantic_sentiment_polarity)\n",
    "        print(\"\\n\")\n",
    "        repeat = False\n",
    "        answer = input('Do you want to analyze another text sentiment? (Yes/No): ').upper()\n",
    "        if answer in ['Y', 'YES', 'YEP', 'YA']:\n",
    "            repeat = True\n",
    "            print(\"-------------------------\")\n",
    "\n",
    "        else:\n",
    "            print ('\\n Thank you! See you later.')\n",
    "    return\n",
    "\n",
    "  \n",
    "def define_complexity_subjectivity_reviews(df_slice,\n",
    "                                          evaluate: bool = False,\n",
    "                                          plot: bool = False):\n",
    "  \n",
    "      \"\"\"\n",
    "      A function to define high sentiment complexity reviews and Low Subjectivity reviews in a slice of dataset. \n",
    "      If evaluate: it evaluates the TopSSA model performance in df_slice \n",
    "      If plot: It plots distribution of low subjectivity reviews vs. high complexity reviews.\n",
    "      \"\"\"\n",
    "      PSS_mean = df_slice[\"topn_PSS\"].mean()\n",
    "      PSS_std = df_slice[\"topn_PSS\"].std()\n",
    "      NSS_mean = df_slice[\"topn_NSS\"].mean()\n",
    "      NSS_std = df_slice[\"topn_NSS\"].std()\n",
    "\n",
    "      # High PSS(NSS)\n",
    "      high_PSS = df_slice[\"topn_PSS\"] > PSS_mean + PSS_std\n",
    "      high_NSS = df_slice[\"topn_NSS\"] > NSS_mean + NSS_std\n",
    "\n",
    "      # Low PSS(NSS)\n",
    "      low_PSS = df_slice[\"topn_PSS\"] < PSS_mean - PSS_std\n",
    "      low_NSS = df_slice[\"topn_NSS\"] < NSS_mean - NSS_std\n",
    "\n",
    "      # High sentiment complexity\n",
    "      high_complexity = high_PSS & high_NSS\n",
    "      df_slice_high_complexity = df_slice[high_complexity]\n",
    "\n",
    "      # low subjectivity\n",
    "      low_subjectivity = low_PSS & low_NSS\n",
    "      df_slice_low_subjectivity = df_slice[low_subjectivity]\n",
    "\n",
    "      if evaluate:\n",
    "          print(\"\\n Number of reviews with high sentiment complexity: \", len(df_slice_high_complexity))\n",
    "          evaluate_model(df_slice_high_complexity[\"sentiment\"],\n",
    "                         df_slice_high_complexity[\"topn_semantic_sentiment_polarity\"])\n",
    "\n",
    "\n",
    "          print(\"\\n Number of reviews with less subjectivity: \", len(df_slice_low_subjectivity))\n",
    "          evaluate_model(df_slice_low_subjectivity[\"sentiment\"], \n",
    "                         df_slice_low_subjectivity[\"topn_semantic_sentiment_polarity\"])\n",
    "          print(\"\\n\")\n",
    "\n",
    "      if plot:\n",
    "          # PLOTTING\n",
    "          # filter positive and negative review based on Target Variable (actual 'y') or 'sentiment' column\n",
    "          actual_pos_filt = df_slice_high_complexity['sentiment'] == 1\n",
    "          actual_neg_filt = df_slice_high_complexity['sentiment'] == 0\n",
    "\n",
    "          actual_pos_low_subjectivity = df_slice_low_subjectivity['sentiment'] == 1\n",
    "          actual_neg_low_subjectivity = df_slice_low_subjectivity['sentiment'] == 0\n",
    "\n",
    "          # plotting Semantic Sentiment Score Position of Actual Negative Reviews \n",
    "          plt.scatter(df_slice_low_subjectivity['topn_NSS'][actual_neg_low_subjectivity], \n",
    "                   df_slice_low_subjectivity['topn_PSS'][actual_neg_low_subjectivity],  \n",
    "                   label='Actual Negetive Low Subjectivity Reviews',\n",
    "                     color='deeppink',\n",
    "                      alpha=0.4 , # set transparency of color\n",
    "                      s=20 # set size of dots\n",
    "                     )\n",
    "\n",
    "          # plotting Semantic Sentiment Score Position of Actual Positive Reviews \n",
    "          plt.scatter(df_slice_low_subjectivity['topn_NSS'][actual_pos_low_subjectivity], \n",
    "                   df_slice_low_subjectivity['topn_PSS'][actual_pos_low_subjectivity],  \n",
    "                   label='Actual Positive Low Subjectivity Reviews',\n",
    "                 color='springgreen',\n",
    "                      alpha=0.2, # set transparency of color\n",
    "                      s=20 # set size of dots\n",
    "                     )\n",
    "\n",
    "          # plotting Semantic Sentiment Score Position of Actual Negative Reviews \n",
    "          plt.scatter(df_slice_high_complexity['topn_NSS'][actual_neg_filt], \n",
    "                   df_slice_high_complexity['topn_PSS'][actual_neg_filt],  \n",
    "                   label='Actual Negetive High Complexity Reviews',\n",
    "                     color='DarkRed',\n",
    "                      alpha=0.4 , # set transparency of color\n",
    "                      s=20 # set size of dots\n",
    "                     )\n",
    "\n",
    "          # plotting Semantic Sentiment Score Position of Actual Positive Reviews \n",
    "          plt.scatter(df_slice_high_complexity['topn_NSS'][actual_pos_filt], \n",
    "                   df_slice_high_complexity['topn_PSS'][actual_pos_filt],  \n",
    "                   label='Actual Positive High Complexity Reviews',\n",
    "                 color='DarkGreen',\n",
    "                      alpha=0.5, # set transparency of color\n",
    "                      s=20 # set size of dots\n",
    "                     )\n",
    "          # naming the x & y axis\n",
    "          plt.xlabel('Predicted Negative Sentiment Score (NSS)')\n",
    "          plt.ylabel('Predicted Positive Sentiment Score (PSS)')\n",
    "\n",
    "\n",
    "          # plotting the bisector\n",
    "          plt.plot([0,0.4], \n",
    "                   [0,0.4], \n",
    "                   alpha=0.5,\n",
    "                   label='Decision Boundry')\n",
    "\n",
    "          # show a legend on the plot\n",
    "          plt.legend()\n",
    "\n",
    "          # giving a title to the graph\n",
    "          plt.title(\"\"\"\n",
    "          Distribution of low subjectivity reviews vs. high complexity reviews\n",
    "          \"\"\")\n",
    "\n",
    "          # To save the result in the same folder\n",
    "          plt.savefig('../reports/figures/low_subjectivity_vs_high_complexity_reviews_on_PSS_NSS_plane.png')\n",
    "\n",
    "          plt.show()\n",
    "\n",
    "      return df_slice_high_complexity, df_slice_low_subjectivity\n",
    "\n",
    "  \n",
    "def explore_high_complexity_reviews(df_slice):  \n",
    "      \"\"\"\n",
    "      A function to plot the distribution of high complexity reviews on the PSS-NSS plane.\n",
    "      Using plotly, this plot let you explore the reviews by hovering over the datapoints.\n",
    "      \"\"\"\n",
    "      df_slice_high_complexity1 =define_complexity_subjectivity_reviews(df_slice)[0]\n",
    "      text_high_complexity = df_slice_high_complexity1['review'].str.wrap(100).str.replace(\"\\n\", \"<br>\"),\n",
    "\n",
    "      fig = px.strip(df_slice_high_complexity1, \n",
    "                     x= \"topn_NSS\", \n",
    "                     y=\"topn_PSS\",\n",
    "                     color= \"sentiment\",\n",
    "                     color_discrete_sequence=['red','green'],\n",
    "                     hover_name = \"tokenized_text_len\",\n",
    "                     hover_data=text_high_complexity,\n",
    "                     height=800,\n",
    "                     width=800)\n",
    "\n",
    "      fig.update_layout(\n",
    "         title = \"Distribution of high complexity reviews on the PSS-NSS plane\",\n",
    "         xaxis_title = \"Negative Sentiment Score (NSS)\",\n",
    "         yaxis_title = \"Positive Sentiment Score (PSS)\",\n",
    "         font = dict(\n",
    "            family = \"Courier New, monospace\",\n",
    "            size = 12,\n",
    "            color = \"#7f7f7f\"\n",
    "         )\n",
    "      )\n",
    "\n",
    "      fig.add_trace(px.line(x=[0.2,0.4], y=[0.2,0.4]).data[0])\n",
    "\n",
    "      fig.show()\n",
    "\n",
    "      return\n",
    "    \n",
    "  \n",
    "def explore_low_subjectivity_reviews(df_slice):\n",
    "      \"\"\"\n",
    "      A function to plot the distribution of low subjectivity reviews on the PSS-NSS plane.\n",
    "      Using plotly, this plot let you explore the reviews by hovering over the datapoints.\n",
    "      \"\"\"\n",
    "      df_slice_low_subjectivity = define_complexity_subjectivity_reviews(df_slice)[1]\n",
    "      text_low_subjectivity = df_slice_low_subjectivity['review'].str.wrap(100).str.replace(\"\\n\", \"<br>\"),\n",
    "\n",
    "\n",
    "      fig = px.strip(df_slice_low_subjectivity, \n",
    "                     x= \"topn_NSS\", \n",
    "                     y=\"topn_PSS\",\n",
    "                     color= \"sentiment\",\n",
    "                     color_discrete_sequence=['green','red'],\n",
    "                     hover_name = \"tokenized_text_len\",\n",
    "                     hover_data=text_low_subjectivity,\n",
    "                     height=700,\n",
    "                     width=700)\n",
    "\n",
    "      fig.update_layout(\n",
    "         title = \"Distribution of low subjectivity reviews on the PSS-NSS plane\",\n",
    "         xaxis_title = \"Negative Sentiment Score (NSS)\",\n",
    "         yaxis_title = \"Positive Sentiment Score (PSS)\",\n",
    "         font = dict(\n",
    "            family = \"Courier New, monospace\",\n",
    "            size = 12,\n",
    "            color = \"#7f7f7f\"\n",
    "         )\n",
    "      )\n",
    "\n",
    "      fig.add_trace(px.line(x=[0,0.2], y=[0,0.2]).data[0])\n",
    "      fig.show()\n",
    "\n",
    "      return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94cf3799",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_26272/390687844.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\visha\\AppData\\Local\\Temp/ipykernel_26272/390687844.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    import Tokenizer,evaluate_model,bow_vectorizer,\u001b[0m\n\u001b[1;37m                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500b2dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
